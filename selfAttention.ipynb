{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use pytorch to code Self-Attention in Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_SelfAttention(nn.Module):\n",
    "    def __init__(self, dimentiona_model=2, row_dimension=0, column_dimension=1):\n",
    "        super().__init__()\n",
    "        self.weight_query = nn.Linear(in_features=dimentiona_model, out_features=dimentiona_model, bias=False)\n",
    "        self.weight_key = nn.Linear(in_features=dimentiona_model, out_features=dimentiona_model, bias=False)\n",
    "        self.weight_value = nn.Linear(in_features=dimentiona_model, out_features=dimentiona_model, bias=False)\n",
    "        self.row_dimension = row_dimension\n",
    "        self.column_dimension = column_dimension\n",
    "\n",
    "    def forward(self, token_encodings):\n",
    "        q = self.weight_query(token_encodings)\n",
    "        k = self.weight_key(token_encodings)\n",
    "        v = self.weight_value(token_encodings)\n",
    "        similarity_scores = torch.matmul(q, k.transpose(dim0=self.row_dimension, dim1=self.column_dimension))\n",
    "        scaled_similarity_scores = similarity_scores / torch.tensor(k.size(self.column_dimension)**0.5)\n",
    "        attention_percents = F.softmax(scaled_similarity_scores, dim=self.column_dimension) #column wise softmax is applied \n",
    "        attention_scores = torch.matmul(attention_percents, v)\n",
    "        return attention_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0100, 1.0641],\n",
       "        [0.2040, 0.7057],\n",
       "        [3.4989, 2.2427]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_matrix = torch.tensor([[1.16, 0.23],\n",
    "                                 [0.57, 1.36],\n",
    "                                 [4.41, -2.16]])\n",
    "torch.manual_seed(42)\n",
    "tsa = Transformer_SelfAttention(dimentiona_model=2, row_dimension=0, column_dimension=1)\n",
    "tsa(encoding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight matrix values for queries, keys and values are as follows:\n",
      "      weight:\n",
      " tensor([[ 0.5406, -0.1657],\n",
      "        [ 0.5869,  0.6496]], grad_fn=<TransposeBackward0>)\n",
      "      key:\n",
      " tensor([[-0.1549, -0.3443],\n",
      "        [ 0.1427,  0.4153]], grad_fn=<TransposeBackward0>)\n",
      "      value:\n",
      " tensor([[ 0.6233,  0.6146],\n",
      "        [-0.5188,  0.1323]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Calculate Queries, Keys and Values are as follows:\n",
      "      Queries:\n",
      " tensor([[ 0.7621, -0.0428],\n",
      "        [ 1.1063,  0.7890],\n",
      "        [ 1.1164, -2.1336]], grad_fn=<MmBackward0>)\n",
      "      Keys:\n",
      " tensor([[-0.1469, -0.3038],\n",
      "        [ 0.1057,  0.3685],\n",
      "        [-0.9914, -2.4152]], grad_fn=<MmBackward0>)\n",
      "      Values:\n",
      " tensor([[ 0.6038,  0.7434],\n",
      "        [-0.3502,  0.5303],\n",
      "        [ 3.8695,  2.4246]], grad_fn=<MmBackward0>)\n",
      "\n",
      "Similarity, Scaled Similarity, Attention Percents, and Attention Scores are as follows:\n",
      "\n",
      "Similarity Scores:\n",
      "tensor([[-0.0990,  0.0648, -0.6523],\n",
      "        [-0.4022,  0.4078, -3.0024],\n",
      "        [ 0.4842, -0.6683,  4.0461]], grad_fn=<MmBackward0>)\n",
      "\n",
      "Scaled Similarity Scores:\n",
      "tensor([[-0.0700,  0.0458, -0.4612],\n",
      "        [-0.2844,  0.2883, -2.1230],\n",
      "        [ 0.3424, -0.4725,  2.8610]], grad_fn=<DivBackward0>)\n",
      "\n",
      "Attention Percents:\n",
      "tensor([[0.3573, 0.4011, 0.2416],\n",
      "        [0.3410, 0.6047, 0.0542],\n",
      "        [0.0722, 0.0320, 0.8959]], grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "Attention Scores:\n",
      "tensor([[1.0100, 1.0641],\n",
      "        [0.2040, 0.7057],\n",
      "        [3.4989, 2.2427]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#print out weight, key, value matrix that creates the queries, keys, values\n",
    "print(f\"\"\"Weight matrix values for queries, keys and values are as follows:\n",
    "      weight:\\n {tsa.weight_query.weight.transpose(0,1)}\n",
    "      key:\\n {tsa.weight_key.weight.transpose(0,1)}\n",
    "      value:\\n {tsa.weight_value.weight.transpose(0,1)}\\n\"\"\") \n",
    "\n",
    "#calculate queries, keys, values\n",
    "print(f\"\"\"Calculate Queries, Keys and Values are as follows:\n",
    "      Queries:\\n {tsa.weight_query(encoding_matrix)}\n",
    "      Keys:\\n {tsa.weight_key(encoding_matrix)}\n",
    "      Values:\\n {tsa.weight_value(encoding_matrix)}\\n\"\"\")   \n",
    " \n",
    "# Compute Queries, Keys, and Values\n",
    "q = tsa.weight_query(encoding_matrix)\n",
    "k = tsa.weight_key(encoding_matrix)\n",
    "v = tsa.weight_value(encoding_matrix)  # Correct usage\n",
    "\n",
    "# Compute Similarity Scores\n",
    "similarity_scores = torch.matmul(q, k.transpose(0, 1))\n",
    "\n",
    "# Compute Scaled Similarity Scores\n",
    "scaling_factor = torch.tensor(k.size(1)**0.5, dtype=torch.float32)\n",
    "scaled_similarity_scores = similarity_scores / scaling_factor\n",
    "\n",
    "# Compute Attention Percents using Softmax\n",
    "attention_percents = F.softmax(scaled_similarity_scores, dim=1)\n",
    "\n",
    "# Compute Attention Scores\n",
    "attention_scores = torch.matmul(attention_percents, v)\n",
    "\n",
    "# Print Results\n",
    "print(f\"\"\"Similarity, Scaled Similarity, Attention Percents, and Attention Scores are as follows:\n",
    "\n",
    "Similarity Scores:\n",
    "{similarity_scores}\n",
    "\n",
    "Scaled Similarity Scores:\n",
    "{scaled_similarity_scores}\n",
    "\n",
    "Attention Percents:\n",
    "{attention_percents}\n",
    "\n",
    "Attention Scores:\n",
    "{attention_scores}\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
