{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch ## torch let's us create tensors and also provides helper functions\n",
    "import torch.nn as nn ## torch.nn gives us nn.module() and nn.Linear()\n",
    "import torch.nn.functional as F # This gives us the softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_MaskSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements Masked Self-Attention mechanism for a Transformer model.\n",
    "    The mask prevents attention to future tokens, ensuring causal attention.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dimentiona_model=2, row_dimension=0, column_dimension=1):\n",
    "        \"\"\"\n",
    "        Initializes the self-attention module with weight matrices for queries, keys, and values.\n",
    "\n",
    "        Args:\n",
    "            dimentiona_model (int): The dimensionality of the input embeddings (default=2).\n",
    "            row_dimension (int): The row index used for tensor operations.\n",
    "            column_dimension (int): The column index used for tensor operations.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Linear transformation layers to create Queries (Q), Keys (K), and Values (V)\n",
    "        self.weight_query = nn.Linear(in_features=dimentiona_model, out_features=dimentiona_model, bias=False)\n",
    "        self.weight_key = nn.Linear(in_features=dimentiona_model, out_features=dimentiona_model, bias=False)\n",
    "        self.weight_value = nn.Linear(in_features=dimentiona_model, out_features=dimentiona_model, bias=False)\n",
    "\n",
    "        # Store row and column dimensions for tensor operations\n",
    "        self.row_dimension = row_dimension\n",
    "        self.column_dimension = column_dimension\n",
    "\n",
    "    def forward(self, token_encodings, mask=None):\n",
    "        \"\"\"\n",
    "        Computes masked self-attention for a given token encoding matrix.\n",
    "\n",
    "        Args:\n",
    "            token_encodings (Tensor): The input token embeddings of shape (sequence_length, d_model).\n",
    "            mask (Tensor, optional): A boolean mask of shape (sequence_length, sequence_length) \n",
    "                                     where True values indicate positions to be masked.\n",
    "\n",
    "        Returns:\n",
    "            attention_scores (Tensor): The output attention values after applying masking.\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute Queries, Keys, and Values\n",
    "        q = self.weight_query(token_encodings)  # Transform input embeddings into query vectors\n",
    "        k = self.weight_key(token_encodings)    # Transform input embeddings into key vectors\n",
    "        v = self.weight_value(token_encodings)  # Transform input embeddings into value vectors\n",
    "\n",
    "        # Compute raw similarity scores (dot product between Q and K^T)\n",
    "        similarity_scores = torch.matmul(q, k.transpose(dim0=self.row_dimension, dim1=self.column_dimension))\n",
    "\n",
    "        # Scale similarity scores by sqrt(d_model) to stabilize gradients\n",
    "        scaled_similarity_scores = similarity_scores / torch.tensor(k.size(self.column_dimension)**0.5, dtype=torch.float32)\n",
    "\n",
    "        # Apply masking (if provided) to prevent attention to future tokens\n",
    "        if mask is not None:\n",
    "            scaled_similarity_scores = scaled_similarity_scores.masked_fill(mask, value=-1e9)  \n",
    "            # `-1e9` ensures that masked values become effectively zero after softmax.\n",
    "\n",
    "        # Compute attention weights using softmax\n",
    "        attention_percents = F.softmax(scaled_similarity_scores, dim=self.column_dimension)\n",
    "\n",
    "        # Compute final attention scores by multiplying attention weights with values (V)\n",
    "        attention_scores = torch.matmul(attention_percents, v)\n",
    "\n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True],\n",
       "        [False, False,  True],\n",
       "        [False, False, False]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a token encoding matrix (sequence_length=3, d_model=2)\n",
    "# Each row represents a token's embedding with 2 feature dimensions\n",
    "encoding_matrix = torch.tensor([[1.16,  0.23],  # Token 1\n",
    "                                [0.57,  1.36],  # Token 2\n",
    "                                [4.41, -2.16]]) # Token 3\n",
    "\n",
    "# Set manual seed to ensure reproducibility of random weight initialization\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Instantiate Transformer Masked Self-Attention module\n",
    "tmsat = Transformer_MaskSelfAttention(dimentiona_model=2, row_dimension=0, column_dimension=1)\n",
    "\n",
    "# Create a lower triangular mask (prevents attending to future tokens)\n",
    "mask = torch.tril(torch.ones(3,3))  # Generates a lower triangular matrix with 1s below the diagonal\n",
    "\n",
    "# Convert to boolean mask: \n",
    "# True (masked positions - upper triangle) and False (allowed positions - lower triangle)\n",
    "mask = mask == 0  \n",
    "\n",
    "# The mask is now:\n",
    "# [[False,  True,  True],   # Token 1 attends only to itself\n",
    "#  [False, False,  True],   # Token 2 attends to itself and Token 1\n",
    "#  [False, False, False]]   # Token 3 attends to all previous tokens\n",
    "\n",
    "# Print the final mask for reference (if needed)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6038,  0.7434],\n",
       "        [-0.0062,  0.6072],\n",
       "        [ 3.4989,  2.2427]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## calculate masked self-attention\n",
    "tmsat(encoding_matrix, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight matrix values for queries, keys, and values are as follows:\n",
      "      weight:\n",
      " tensor([[ 0.5406, -0.1657],\n",
      "        [ 0.5869,  0.6496]], grad_fn=<TransposeBackward0>)\n",
      "      key:\n",
      " tensor([[-0.1549, -0.3443],\n",
      "        [ 0.1427,  0.4153]], grad_fn=<TransposeBackward0>)\n",
      "      value:\n",
      " tensor([[ 0.6233,  0.6146],\n",
      "        [-0.5188,  0.1323]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Calculate Queries, Keys, and Values are as follows:\n",
      "      Queries:\n",
      " tensor([[ 0.7621, -0.0428],\n",
      "        [ 1.1063,  0.7890],\n",
      "        [ 1.1164, -2.1336]], grad_fn=<MmBackward0>)\n",
      "      Keys:\n",
      " tensor([[-0.1469, -0.3038],\n",
      "        [ 0.1057,  0.3685],\n",
      "        [-0.9914, -2.4152]], grad_fn=<MmBackward0>)\n",
      "      Values:\n",
      " tensor([[ 0.6038,  0.7434],\n",
      "        [-0.3502,  0.5303],\n",
      "        [ 3.8695,  2.4246]], grad_fn=<MmBackward0>)\n",
      "\n",
      "Similarity, Scaled Similarity, Attention Percents, and Attention Scores are as follows:\n",
      "\n",
      "Similarity Scores:\n",
      "tensor([[-0.0990,  0.0648, -0.6523],\n",
      "        [-0.4022,  0.4078, -3.0024],\n",
      "        [ 0.4842, -0.6683,  4.0461]], grad_fn=<MmBackward0>)\n",
      "\n",
      "Scaled Similarity Scores (before masking):\n",
      "tensor([[-0.0700,  0.0458, -0.4612],\n",
      "        [-0.2844,  0.2883, -2.1230],\n",
      "        [ 0.3424, -0.4725,  2.8610]], grad_fn=<DivBackward0>)\n",
      "\n",
      "Mask Used:\n",
      "tensor([[False,  True,  True],\n",
      "        [False, False,  True],\n",
      "        [False, False, False]])\n",
      "\n",
      "Scaled Similarity Scores (after masking applied):\n",
      "tensor([[-6.9975e-02, -1.0000e+09, -1.0000e+09],\n",
      "        [-2.8442e-01,  2.8833e-01, -1.0000e+09],\n",
      "        [ 3.4241e-01, -4.7253e-01,  2.8610e+00]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "\n",
      "Attention Percentages (after Softmax):\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.3606, 0.6394, 0.0000],\n",
      "        [0.0722, 0.0320, 0.8959]], grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "Attention Scores:\n",
      "tensor([[ 0.6038,  0.7434],\n",
      "        [-0.0062,  0.6072],\n",
      "        [ 3.4989,  2.2427]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Print weight matrices for Queries, Keys, and Values\n",
    "print(f\"\"\"Weight matrix values for queries, keys, and values are as follows:\n",
    "      weight:\\n {tmsat.weight_query.weight.transpose(0,1)}\n",
    "      key:\\n {tmsat.weight_key.weight.transpose(0,1)}\n",
    "      value:\\n {tmsat.weight_value.weight.transpose(0,1)}\\n\"\"\") \n",
    "\n",
    "# Compute Queries, Keys, and Values\n",
    "q = tmsat.weight_query(encoding_matrix)\n",
    "k = tmsat.weight_key(encoding_matrix)\n",
    "v = tmsat.weight_value(encoding_matrix)  # Correct usage\n",
    "\n",
    "# Print Queries, Keys, and Values\n",
    "print(f\"\"\"Calculate Queries, Keys, and Values are as follows:\n",
    "      Queries:\\n {q}\n",
    "      Keys:\\n {k}\n",
    "      Values:\\n {v}\\n\"\"\")   \n",
    "\n",
    "# Compute Similarity Scores\n",
    "similarity_scores = torch.matmul(q, k.transpose(0, 1))\n",
    "\n",
    "# Compute Scaled Similarity Scores\n",
    "scaling_factor = torch.tensor(k.size(1)**0.5, dtype=torch.float32)\n",
    "scaled_similarity_scores = similarity_scores / scaling_factor\n",
    "\n",
    "# Apply Masking\n",
    "masked_scaled_similarity_scores = scaled_similarity_scores.masked_fill(mask, value=-1e9)  # Masking future tokens\n",
    "\n",
    "# Compute Attention Percents using Softmax\n",
    "attention_percents = F.softmax(masked_scaled_similarity_scores, dim=1)\n",
    "\n",
    "# Compute Attention Scores\n",
    "attention_scores = torch.matmul(attention_percents, v)\n",
    "\n",
    "# Print Results\n",
    "print(f\"\"\"Similarity, Scaled Similarity, Attention Percents, and Attention Scores are as follows:\n",
    "\n",
    "Similarity Scores:\n",
    "{similarity_scores}\n",
    "\n",
    "Scaled Similarity Scores (before masking):\n",
    "{scaled_similarity_scores}\n",
    "\n",
    "Mask Used:\n",
    "{mask}\n",
    "\n",
    "Scaled Similarity Scores (after masking applied):\n",
    "{masked_scaled_similarity_scores}\n",
    "\n",
    "Attention Percentages (after Softmax):\n",
    "{attention_percents}\n",
    "\n",
    "Attention Scores:\n",
    "{attention_scores}\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
