# ğŸ”¥ Transformer Self-Attention & Masked Self-Attention (PyTorch)

This repository implements **Self-Attention** and **Masked Self-Attention** in PyTorch, essential components of the **Transformer architecture** (Vaswani et al., 2017). ğŸš€

- âœ… Implemented **Self-Attention Mechanism**
- âœ… Implemented **Masked Self-Attention (for causal attention)**
- âœ… Included **matrix operations for Q, K, V computations**
- âœ… Supports **custom input encodings and masking**
- âœ… Written in **PyTorch** for easy extensibility

---

## ğŸ” **Understanding Self-Attention**

Self-attention allows a model to weigh different parts of the input sequence dynamically.

### **ğŸ‘‰ Self-Attention Formula:**

\[
\text{Attention}(Q, K, V) = \text{softmax} \left(\frac{QK^T}{\sqrt{d\_{\text{model}}}} \right) V
\]

Each input token generates:

- **Query (Q)** â†’ Represents what it is looking for.
- **Key (K)** â†’ Represents what it contains.
- **Value (V)** â†’ Represents actual token embeddings.

---

## ğŸ›  **Implementation Details**

This repository contains two key implementations:

### **1ï¸âƒ£ Self-Attention (`Transformer_SelfAttention`)**

ğŸ“Œ Computes **attention scores** over the entire sequence.

#### **Code Example**

```python
tsa = Transformer_SelfAttention(dimentiona_model=2, row_dimension=0, column_dimension=1)
tsa(encoding_matrix)
```
