# ğŸ”¥ Transformer Self-Attention & Masked Self-Attention (PyTorch)

This repository implements **Self-Attention** and **Masked Self-Attention** in PyTorch, essential components of the **Transformer architecture** (Vaswani et al., 2017). ğŸš€

- âœ… Implemented **Self-Attention Mechanism**
- âœ… Implemented **Masked Self-Attention (for causal attention)**
- âœ… Included **matrix operations for Q, K, V computations**
- âœ… Supports **custom input encodings and masking**
- âœ… Written in **PyTorch** for easy extensibility

---

## ğŸ” **Understanding Self-Attention**

Self-attention allows a model to weigh different parts of the input sequence dynamically.

### **ğŸ‘‰ Self-Attention Formula:**

\[
\text{Attention}(Q, K, V) = \text{softmax} \left(\frac{QK^T}{\sqrt{d\_{\text{model}}}} \right) V
\]

Each input token generates:

- **Query (Q)** â†’ Represents what it is looking for.
- **Key (K)** â†’ Represents what it contains.
- **Value (V)** â†’ Represents actual token embeddings.

---

## ğŸ›  **Implementation Details**

This repository contains two key implementations:

### **1ï¸âƒ£ Self-Attention (`Transformer_SelfAttention`)**

ğŸ“Œ Computes **attention scores** over the entire sequence.

#### **Code Example**

```python
tsa = Transformer_SelfAttention(dimentiona_model=2, row_dimension=0, column_dimension=1)
tsa(encoding_matrix)
```

### **2ï¸âƒ£ Masked Self-Attention (`Transformer_MaskSelfAttention`)**
ğŸ“Œ Used in decoder models (GPT, Transformer Decoder) to prevent attention to future tokens.

---

## ğŸ”’ **Causal Masking**

A lower-triangular mask is used:

```python
[[False,  True,  True],
 [False, False,  True],
 [False, False, False]]
```

### **Meaning:**
- **Token 1** attends only to itself.
- **Token 2** attends to itself + previous tokens.
- **Token 3** attends to itself + all previous tokens.

#### **Code Example**

```python
mask = torch.tril(torch.ones(3,3)) == 0  # Lower triangular mask
tmsat(encoding_matrix, mask)  # Apply masked self-attention
```

---

## ğŸ“‚ **Project Structure**

```bash
ğŸ“¦ Transformer-SelfAttention
â”œâ”€â”€ ğŸ“œ self_attention.py          # Self-Attention Implementation
â”œâ”€â”€ ğŸ“œ masked_self_attention.py   # Masked Self-Attention Implementation
â”œâ”€â”€ ğŸ“œ demo.ipynb                 # Jupyter Notebook with Examples
â”œâ”€â”€ ğŸ“œ README.md                  # Documentation
```

---

## ğŸš€ **Run the Code**

### ğŸ“Œ Install Dependencies

```bash
pip install torch numpy
```

### ğŸ“Œ Run Self-Attention

```bash
python self_attention.py
```

### ğŸ“Œ Run Masked Self-Attention

```bash
python masked_self_attention.py
```

---

## ğŸ“Š **Results & Visualizations**

Self-Attention and Masked Self-Attention results include:

- Query, Key, Value transformations
- Similarity Scores
- Attention Scores
- Masking Effect (Masked Self-Attention)

### **Example output:**

```python
Similarity Scores:
tensor([[ 2.35, -0.56,  4.12],
        [ 1.89,  3.44, -1.23],
        [-0.23,  1.78,  5.89]])

Scaled Similarity Scores (after sqrt(d_model)):
tensor([[ 1.67, -0.40,  2.91],
        [ 1.34,  2.43, -0.87],
        [-0.16,  1.26,  4.18]])

Attention Scores:
tensor([[ 1.92,  0.47],
        [ 2.15, -0.65],
        [-1.78,  3.05]])
```

